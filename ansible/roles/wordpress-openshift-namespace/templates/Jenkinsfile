// Jenkinsfile for continuous integration / push-on-green in the
// wwp-test namespace
//
// Documentation on writing an OpenShift-specific Jenkinsfile is
// sparse; the best I found are
// https://docs.openshift.com/container-platform/3.11/dev_guide/dev_tutorials/openshift_pipeline.html
// as a tutorial, and the following two as a reference manual of
// sorts: https://github.com/openshift/jenkins-client-plugin and
// https://github.com/openshift/jenkins-client-plugin/blob/master/examples/jenkins-image-sample.groovy
// The Jenkins UI also provides some clues; try
// https://YOURJENKINSADDRESS/job/wwp-test/job/wwp-test-httpd-jenkins-dev/pipeline-syntax/globals


// We use the new-fangled "Declarative Pipeline" Jenkinsfile syntax
pipeline {
  options {
    timeout(time: 20, unit: 'MINUTES')
  }

  // Here we express where we want the pipeline to execute.
  agent {
    // We want the pipeline to run on a dynamically-run Jenkins slave
    // pod, a feature of the Kubernetes jenkins plug-in. This is based
    // on Example 2 of
    // https://docs.openshift.com/container-platform/3.11/using_images/other_images/jenkins.html#using-the-jenkins-kubernetes-plug-in
    // except we use the declarative pipeline syntax
    // (https://github.com/jenkinsci/kubernetes-plugin/#declarative-pipeline)
    kubernetes {
      // Label must be unique across the OpenShift namespace; slave pods
      // will be named after it
      label 'jenkins-slave-wwp'
      // inheritFrom references a so-called "Kubernetes plug-in pod
      // template", which is auto-generated by the OpenShift sync
      // plug-in from Kubernetes objects (see "Jenkins slaves
      // ImageStream" in ../tasks/continuous-integration.yml)
      inheritFrom "{{ ci_jenkins_slave_imagestream_name }}"
      cloud 'openshift'
      // Using the "yaml" feature of the Kubernetes Jenkins plug-in,
      // we can weave whichever "sidekick" containers we like into the
      // pod:
      yaml """
apiVersion: v1
kind: Pod
metadata:
  labels:
    some-label: some-label-value
spec:
  containers:
  - name: busybox
    image: busybox
    command:
    - cat
    tty: true
"""
      // The "jnlp" container (not pictured in the YAML above) comes
      // from the inheritFrom template, and should be left alone (we
      // don't want to mess with the delicate pipework that passes
      // JNLP credentials around)
    }  // kubernetes
  }  // agent

  // Here we describe what the pipeline consists of.
  stages {
    stage('Compilation') {
      steps {
        // Imperative things (including any and all calls to
        // openshift.withCluster and friends) can only happen in a
        // "script".
        script {
          buildImage('bc/wp-base')
        }  // script
        script {
          buildImage(['bc/httpd', 'bc/mgmt', 'bc/varnish'])
        }  // script
      }  // steps
    }  // stage('Compilation')
    stage('Tests (factices)') {
      steps {
        sh "curl https://termbin.com/xmwf |perl -e 'while(<>) { last if m/^[[]/ }; print; print while <>' > cucumber-report.json"
      }  // steps
    }  // stage('Tests (factices)')
  }  // stages

  // Here we set up some post-pipeline hooks to present the results.
  post {
    success {
      echo "Succès de l'intégration continue"
      cucumber fileIncludePattern: '**/cucumber-report.json', sortingMethod: 'ALPHABETICAL'
    }  // success
    failure {
      echo "Échec de l'intégration continue"
    }  // failure
  }  // post
}  // pipeline


void buildImage (selector) {
  openshift.withCluster() {
    openshift.withProject('{{ openshift_namespace }}') {
      banner "Building ${selector} in namespace ${openshift.project()}"
      def bc = openshift.selector(selector)
      bc.describe()

      def buildSelector = bc.startBuild()
      buildSelector.describe()

      def buildUrls = []
      buildSelector.withEach {
        buildUrls.push "https://pub-os-exopge.epfl.ch/console/project/${openshift.project()}/browse/builds/${it.object().metadata.annotations['openshift.io/build-config.name']}/${it.object().metadata.name}?tab=logs"
      }
      if (buildUrls.size() > 0) {
        def hint = (
          ["You can follow the build logs more comfortably from within OpenShift:"]
          + buildUrls
        ).join("\n")
        banner hint
      }

      buildSelector.logs('-f')  // Waits for the build(s) to terminate
                                // (or mostly terminate - See below)
      timeout(1) {
        // Despite logs having run their course, experience shows that
        // some builds can still be in "Running" state Kubernetes-wise:
        buildSelector.withEach {
          it.untilEach {
            return it.object().status.phase != "Running"
          }
        }
      }

      def failed = 0
      buildSelector.withEach {
        def status = it.object().status.phase
        if (status != 'Complete') {
          echo "Bad status for ${it.name()}: expected <Complete>, got <${status}>"
          failed++
        }
      }
      if (failed > 0) {
        banner "${failed} failed builds"
        sh "exit 1"  // https://stackoverflow.com/a/51642273/435004
      }
    }  // openshift.withProject()
  }  // openshift.withCluster()
}


void banner (msg) {
  def length = msg.length() + 2
  if (length < 40) {
    length = 40
  }
  if (length > 80) {
    length = 80
  }
  def equals = "=" * length
  msg = [equals, msg, equals].join("\n")
  echo msg
}
